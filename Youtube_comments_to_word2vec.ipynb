{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Youtube comments to word2vec\n",
    "==="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a basic implementation of word2vec using youtube comments as the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import zipfile\n",
    "import tensorflow as tf\n",
    "import json\n",
    "import collections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Read data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 7364\n",
      "Comments preview:\n",
      " [['#FixRussianYoutube', 'Philip', 'we', 'urgently', 'need', 'your', 'help!', 'Please', 'consider', 'a', 'situation', 'abour', 'Russian', 'YouTube,', 'which', 'helps', 'russian', 'government', 'to', 'delete', 'videos', 'of', 'opposition.', 'And', 'not', 'only', 'delete', 'videos,', 'but', 'also', 'prevents', 'videos', 'to', 'get', 'in', '\"Trending', 'videos\"', 'by', 'freezing', 'views', '(', \"i'm\", 'not', 'talking', 'about', 'situations', 'when', 'views', 'stopsfor', 'several', 'minutes,', \"i'm\", 'talking', 'about', 'when', 'views', 'hold', 'on', 'a', 'same', 'number', 'for', 'whole', 'day!).', 'Spread', 'this', 'information,', 'we', 'need', 'your', 'help!'], ['I', 'am', 'so', 'upset', 'about', 'Chester.', \"I've\", 'been', 'listening', 'to', 'Linkin', 'Park', 'since', 'the', 'beginning,', 'like', 'a', 'lot', 'of', 'fans.', 'It', 'saddens', 'me', 'I', 'will', 'never', 'hear', 'a', 'new', 'Linkin', 'Park', 'song', 'again.', 'And', 'to', 'the', 'people', 'wishing', 'death', 'on', 'McCain,', 'they', 'really', 'need', 'to', 'reevaluate', 'themselves.', \"It's\", 'horrible', 'and', 'despicable', 'that', 'they', 'want', 'someone', 'dead', 'because', 'they', \"don't\", 'agree', 'with', 'them.', 'There', 'are', 'friends', 'that', 'I', \"don't\", 'agree', 'with', 'many', 'times', 'but', 'I', \"don't\", 'want', 'them', 'dead.', 'Disgusting', 'garbage', 'people', 'as', 'you', 'said.'], [\"He's\", 'obviously', 'not', 'a', 'big', 'fan', 'of', 'LP', '..or..', 'he', 'jist', \"couldn't\", 'talk', 'much', 'about', 'it', 'afraid', 'to', 'ball', 'his', 'eyes', 'on', 'the', 'matter,', 'making', 'it', 'a', 'short', 'one.', 'hm.'], ['I', 'understand', 'cop', 'news', 'goes', 'viral', 'when', \"it's\", 'possible', 'they', 'did', 'something', 'wrong.', 'When', 'they', 'do', 'good,', 'no', 'one', 'cares', 'because', \"it's\", 'not', 'what', 'the', 'public', 'wants', 'to', 'hear', 'right', 'now.', 'Phil,', 'can', 'you', 'once', 'in', 'a', 'while', 'cover', 'something', 'good', 'they', 'do', 'since', 'no', 'one', 'else', 'will?', 'No', 'mater', 'what,', 'if', 'I', 'need', 'the', 'cops,', \"I'm\", 'calling', 'for', 'the', 'cops', 'no', 'matter', 'what', 'kinda', 'BS', 'is', 'in', 'the', 'news', 'about', 'them', 'and', \"I'm\", 'sure', 'the', 'public', 'feels', 'the', 'same.'], ['imagine', 'how', 'many', 'things', 'they', 'have', 'gotten', 'away', 'with...']]\n"
     ]
    }
   ],
   "source": [
    "#Note: Comments can also be seen as documents and vice versa\n",
    "\n",
    "def read_comments_from_json(filename):\n",
    "    \"\"\"Read youtube comments from json file and return a list of each comment as a list of words.\"\"\"  \n",
    "    comments_vocabulary = []\n",
    "    filename += '.json' # Add extension to filename\n",
    "    file_path = os.path.join('.', filename)\n",
    "    with open(file_path) as file:\n",
    "        data = json.load(file) #deserialize to python object\n",
    "        for comment in data:\n",
    "            # Each comment will represent a document which can be used to implement Doc2Vec\n",
    "            comments_vocabulary.append(comment[\"commentText\"].split())\n",
    "    return comments_vocabulary\n",
    "\n",
    "\n",
    "filename = 'comments'\n",
    "comments_vocabulary = read_comments_from_json(filename)\n",
    "data_size = len(comments_vocabulary)\n",
    "print('Data size', data_size)\n",
    "print('Comments preview:\\n', comments_vocabulary[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Build the dictionary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common words (+UNK) in first 5 documents [[['UNK', 52], ('views', 3), ('we', 2), ('videos', 2), ('not', 2), ('talking', 2), ('help!', 2), ('about', 2), ('a', 2), ('need', 2)], [['UNK', 61], ('I', 4), ('they', 3), ('to', 3), (\"don't\", 3), ('Linkin', 2), ('that', 2), ('with', 2), ('a', 2), ('agree', 2)], [['UNK', 19], ('a', 2), ('it', 2), ('making', 1), ('talk', 1), ('hm.', 1), ('on', 1), ('matter,', 1), ('not', 1), ('to', 1)], [['UNK', 56], ('the', 6), ('no', 3), ('they', 3), ('something', 2), ('I', 2), ('public', 2), ('do', 2), ('one', 2), ('what', 2)], [['UNK', 0], ('away', 1), ('how', 1), ('imagine', 1), ('gotten', 1), ('they', 1), ('have', 1), ('with...', 1), ('many', 1), ('things', 1)]]\n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = 10 # This number will depend on the max size of a youtube comment\n",
    "\n",
    "def build_dataset(documents_vocabulary, n_words):\n",
    "    \"\"\"Process raw inputs into datasets\"\"\"\n",
    "    documents_dataset = {'data': list(), 'count': list(), 'dictionary': list(), 'reversed_dictionary': list()}\n",
    "    for words in documents_vocabulary:\n",
    "        count = [['UNK', -1]] #Keeps track of common terms and unknow terms along with their count\n",
    "        count.extend(collections.Counter(words).most_common(n_words - 1)) # extends \"count\" by adding the n_words (n most common words) found in each document\n",
    "        dictionary = dict() #Keeps track of words found in count along with their id. \n",
    "        for word, _ in count:\n",
    "            dictionary[word] = len(dictionary)\n",
    "        data = list() #keeps track of the id of the words that appear in the dictinary in the order they appear in the vocabulary\n",
    "        unk_count = 0\n",
    "        for word in words:\n",
    "            if word in dictionary:\n",
    "                index = dictionary[word]\n",
    "            else:\n",
    "                index = 0\n",
    "                unk_count += 1\n",
    "            data.append(index)\n",
    "        count[0][1] = unk_count # updata 'UNK' to reflect the number of unknown terms found so far\n",
    "        reversed_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "        documents_dataset['data'].append(data)\n",
    "        documents_dataset['count'].append(count)\n",
    "        documents_dataset['dictionary'].append(dictionary)\n",
    "        documents_dataset['reversed_dictionary'].append(reversed_dictionary)\n",
    "        \n",
    "    return documents_dataset\n",
    "    \n",
    "documents_dataset = build_dataset(comments_vocabulary, vocabulary_size)\n",
    "\n",
    "del comments_vocabulary #Not needed so delete to reduce memory    \n",
    "print('Most common words (+UNK) in first 5 documents', documents_dataset['count'][:5])\n",
    "#print('Keys', documents_dataset.keys())\n",
    "#print('Values', documents_dataset.values())\n",
    "\n",
    "data_index = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generate training batch (Skip-gram model)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-58-3316fb91e14a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m \u001b[0mdocuments_batches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-58-3316fb91e14a>\u001b[0m in \u001b[0;36mgenerate_batch\u001b[0;34m(batch_size, num_skips, skip_window)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m#TODO: Take care of documents that don't contain enough data. Some documents could be single word\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspan\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m                 \u001b[0mbuffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m                 \u001b[0mdata_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata_index\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mnum_skips\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "def generate_batch(batch_size, num_skips, skip_window):\n",
    "    documents_batches = {'batch': list(), 'labels': list()}\n",
    "    global data_index\n",
    "    assert batch_size % num_skips == 0\n",
    "    assert num_skips <= 2 * skip_window\n",
    "    batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "    labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "    span = 2* skip_window + 1 # [skip_window target skip_window]\n",
    "    buffer = collections.deque(maxlen=span) #Keeps track all words being analized during each iteration\n",
    "\n",
    "    for document_n in range(data_size):\n",
    "        \n",
    "        data = documents_dataset['data'][document_n]\n",
    "        if(len(data) > 10): #TODO: Take care of documents that don't contain enough data. Some documents could be single word\n",
    "            for _ in range(span):\n",
    "                buffer.append(data[data_index])\n",
    "                data_index = (data_index + 1) % len(data)\n",
    "            for i in range(batch_size // num_skips):\n",
    "                target = skip_window # target label at the center of the buffer\n",
    "                targets_to_avoid = [skip_window]\n",
    "                for j in range(num_skips):\n",
    "                    while target in targets_to_avoid:\n",
    "                        target = random.randint(0, span - 1) \n",
    "                    targets_to_avoid.append(target)\n",
    "                    batch[i * num_skips + j] = buffer[skip_window]\n",
    "                    labels[i* num_skips + j, 0] = buffer[target]\n",
    "\n",
    "                buffer.append(data[data_index])\n",
    "                data_index = (data_index + 1) % len(data)\n",
    "\n",
    "            data_index = (data_index + len(data) - span) % len(data)\n",
    "            documents_batches['batch'].append(batch)\n",
    "            documents_batches['labels'].append(labels)\n",
    "        \n",
    "    return documents_batches\n",
    "        \n",
    "            \n",
    "documents_batches = generate_batch(8, 2, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note to self:** Fix problem with \"list index out of range\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
